{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reproducibly Capturing Code\n",
    "\n",
    "The Python data science ecosystem is broad, ever expanding and constantly evolving. This dynamic nature makes Python an excellent way to use state of the art data science tools but it can also make it hard to reproduce previously completed work as libaries and their APIs keep changing.\n",
    "\n",
    "This notebook will demonstrate an approach to tame this complexity in order to reproducibly capture code together with artefacts such as notebooks. In particular, we will use a tool called [anaconda project](https://github.com/Anaconda-Platform/anaconda-project) to specify reproducible conda environments together with associated assets and executable command invocations. Taken together, these are called 'projects' where a project is a self-contained, reproducible unit of work that can be shared with colleagues or archived in the knowledge that they can be reproducibly executed at a later date.\n",
    "\n",
    "First, we will use an example notebook to illustrate the problems commonly faced by a user when they find a notebook online that they want to then run and execute.\n",
    "\n",
    "\n",
    "## Illustrating the problem\n",
    "\n",
    "To illustrate the problem, we will try to execute the `Exploring_Data.ipynb` notebook described in the 'Exploring Data in Notebooks' talk. This notebook can be found in the archive that you can download [here](https://anaconda.org/jlstevens/project/exploring-data).\n",
    "\n",
    "We will now follow all the steps that a user might have to run before they can run the notebook from top to bottom in an empty conda environment.\n",
    "\n",
    "### 1. Creating an empty environment\n",
    "\n",
    "The first step is to use conda to create a new empty environment that we shall call `empty`:\n",
    "\n",
    "\n",
    "```bash\n",
    "$ conda create -n empty python\n",
    "```\n",
    "\n",
    "Once Python and the minimal set of dependencies are installed into the new environment, we can activate it:\n",
    "\n",
    "```bash\n",
    "$ conda activate empty\n",
    "```\n",
    "\n",
    "### 2. Installing and running the notebook server\n",
    "\n",
    "At this point we realize that we cannot view the notebook without installing Jupyter notebook. We can now install `notebook` into our conda environment and run the `jupyter notebook` command to view it:\n",
    "\n",
    "```bash\n",
    "$ conda install notebook\n",
    "$ jupyter notebook Exploring_Data.ipynb\n",
    "```\n",
    "\n",
    "Now we can view the notebook, but immediately we hit an error upon executing the first code cell:\n",
    "\n",
    "```python\n",
    "ModuleNotFoundError: No module named 'pandas'\n",
    "```\n",
    "\n",
    "Now we need to start populating the environment with the libraries needed by the notebook.\n",
    "\n",
    "### 3. Install directly imported dependencies\n",
    "\n",
    "Conda installing each imported libraries one at a time is tedious, so at this point we might scan the notebook to see which libraries are imported with the Python `import` statement. Doing this we spot `pandas`,`numpy`,`hvplot`, `holoviews`, `xarray`.\n",
    "\n",
    "At this point we may use our knowledge that `hvplot` is built on `holoviews` to realize that installing `hvplot` will also install `holoviews`. We can therefore run the following command with or without specifying `holoviews`:\n",
    "\n",
    "```bash\n",
    "$ conda install pandas numpy hvplot holoviews xarray\n",
    "```\n",
    "\n",
    "### 4. Install optional dependencies\n",
    "\n",
    "At this point, we might think that everything is ready to go. However, if we rerun the notebook in the updated environment we hit two new errors.\n",
    "\n",
    "1. ```python\n",
    "   import hvplot.dask\n",
    "   ...\n",
    "   ModuleNotFoundError: No module named 'dask'\n",
    "```\n",
    "\n",
    "2. ```python\n",
    "   from hvplot.sample_data import airline_flights\n",
    "   ...\n",
    "   ModuleNotFoundError: No module named 'intake'\n",
    "   ImportError: Loading hvPlot sample data requires intake and intake-parquet. Install it using conda or pip before loading data.\n",
    "   ```\n",
    "   \n",
    "These dependencies were not satisfied by the previous step as these dependencies are *optional*. In particular, if you want to use `hvplot` with `dask` you will need `dask` (even if you don't, `dask` would have be needed to run the line `flights = airline_flights.to_dask().persist()`). Secondly, if you want to import the `airline_flights` example from `hvplot.sample_data`, you need the `intake` library.\n",
    "\n",
    "Noting the mention of `intake-parquet` as well in the `ModuleNotFoundError` message, we can now run:\n",
    "\n",
    "```bash\n",
    "$ conda install dask intake intake-parquet\n",
    "```\n",
    "\n",
    "### 5. Install nested optional dependencies\n",
    "\n",
    "Optional dependencies may themselves have optional dependencies. The following line first complained about \"No module named 'intake'\" but now it raises a different exception:\n",
    "\n",
    "```python\n",
    "from hvplot.sample_data import airline_flights\n",
    "...\n",
    "ValueError: No plugins loaded for this entry: xarray_image\n",
    "```\n",
    "\n",
    "At this point, we need to know intake well enough to know that the request for the `xarray_image` plugin is satisfied by the `intake-xarray` package. Now we need:\n",
    "\n",
    "```bash\n",
    "$ conda install intake-xarray\n",
    "```\n",
    "\n",
    "### 6. Install dynamic dependencies\n",
    "\n",
    "At this point we might hope that all the dependencies are now satisfied and the line `from hvplot.sample_data import airline_flights` does finally execute without error! However, we are not done:\n",
    "\n",
    "\n",
    "```python\n",
    "flights = airline_flights.to_dask().persist()\n",
    "flights.head()\n",
    "...\n",
    "RuntimeError: Decompression 'SNAPPY' not available.  Options: ['BROTLI', 'GZIP', 'UNCOMPRESSED']\n",
    "```\n",
    "\n",
    "Once again, we need to know how to address this message appropriately by making 'snappy' compression available. The required packages is called `python-snappy`:\n",
    "\n",
    "```bash\n",
    "$ conda install python-snappy\n",
    "```\n",
    "\n",
    "Surely we are done conda installing?\n",
    "\n",
    "### 7. Install chained nested dependencies\n",
    "\n",
    "Unfortunately, we are not done due to one final error:\n",
    "\n",
    "```python\n",
    "flights.hvplot.scatter(x='distance', y='airtime', datashade=True)\n",
    "...\n",
    "ImportError: Datashading is not available\n",
    "```\n",
    "\n",
    "At this point we realize that `datashader` needs to be installed to use the `datashade=True` flag. Without carefully thinking about all the code in the notebook, it is tricky to realize this requirement earlier as it wasn't possible to create the `flights` DataFrame due to all the previous errors regarding missing dependencies. Finally we can conclude by running:\n",
    "\n",
    "```bash\n",
    "$ conda install datashader\n",
    "```\n",
    "\n",
    "## Possible solutions\n",
    "\n",
    "It is clear that getting even a relatively simple notebook running without suitable instructions can be a painful process! As a notebook author, you want people to be able to easily run your code for themselves. Here are some commonly used ways to address this problem.\n",
    "\n",
    "\n",
    "### Inline instructions\n",
    "\n",
    "The simplest approach may be to collect the necessary conda install commands together and give instructions at the top of your notebook such as:\n",
    "\n",
    "```bash\n",
    "# To run this notebook, run it as follows:\n",
    "$ conda create -n notebook-env python\n",
    "$ conda activate notebook-env\n",
    "$ conda install notebook pandas numpy hvplot holoviews xarray dask intake intake-parquet intake-xarray python-snappy datashader\n",
    "$ jupyter notebook Exploring_Data.ipynb\n",
    "```\n",
    "\n",
    "This is rather a lot of lines for a user to follow and it is not robust: none of the versions are pinned which means that any new versions of these libraries might introduce API changes that break the notebook.\n",
    "\n",
    "\n",
    "### Supplying an `environment.yml`\n",
    "\n",
    "Another common approach is to ship the notebook with an `environment.yaml` file containing all the necessary dependencies with version pins. While this results in a more reproducible environment that the instructions above, there are still several disadvantages:\n",
    "\n",
    "1. You now have an extra file that is associated with the notebook that you need to ship to your users.\n",
    "2. The environment name may clash with existing environment names and these environment persist. If you have a hundred projects, you need a hundred different environments!\n",
    "3. The `environment.yml` file constantly needs to be updated as the notebook evolves. Testing that the `environment.yml` file is working involves constant testing of the notebook in freshly regenerated environments.\n",
    "4. The command to run the notebook `jupyter notebook Exploring_Data.ipynb` still needs to be captured somewhere, possibly in the notebook itself. \n",
    "\n",
    "### Using `anaconda-project`\n",
    "\n",
    "The [anaconda project](https://github.com/Anaconda-Platform/anaconda-project) utility addresses all the problems listed above with `environment.yml` and also avoids requiring the user to run lots of instructions. With `anaconda project`, notebooks, file assets, environments and commands can all be packaged together into a single file that can be easily distributed.\n",
    "\n",
    "The only requirement to use `anaconda project` (by both project authors and users) is the following command necessary to install `anaconda project` into any existing environment:\n",
    "\n",
    "```bash\n",
    "$ conda install anaconda-project\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First cut at `anaconda-project.yml`\n",
    "\n",
    "An `anaconda-project.yml` file declares all the information needed by `anaconda project`. With the list of necessary packages identified above, we can have a first cut at this file:\n",
    "\n",
    "```yaml\n",
    "name: exploring-data\n",
    "description: If you have a Pandas or Dask dataframe or an Xarray data\n",
    "             array, you can now get fully interactive, composable plots as simply as\n",
    "             \"df.hvplot()\".  We'll show how to use these tools for rapidly exploring\n",
    "             dastasets in Jupyter so that you can quickly spot outliers and\n",
    "             inconsistencies while revealing the big picture.\n",
    "\n",
    "channels: []\n",
    "\n",
    "packages:\n",
    "  - python\n",
    "  - notebook\n",
    "  - hvplot\n",
    "  - intake\n",
    "  - intake-parquet\n",
    "  - intake-xarray\n",
    "  - s3fs\n",
    "  - datashader\n",
    "  - python-snappy\n",
    "\n",
    "\n",
    "env_specs:\n",
    "  default: {}\n",
    "```\n",
    "\n",
    "Most fields here are self-explanatory: `name` is a short named given to the project while `description` allows for a long description of the project's aims and goals. `channels` lets you specify additional conda channels to draw packages from such as `conda-forge` (if necessary). Finally `env_specs`  defines one or more environments where we only need a single environment called `default` to install packages into.\n",
    "\n",
    "This this saved as `anaconda-project.yaml` in the directory where `Exploring_Data.ipynb`, `Reproducibly_Capturing_Code.ipynb` and `diseases.csv.gz` reside, we can now prepare the environment using:\n",
    "\n",
    "```bash\n",
    "$ anaconda project prepare\n",
    "```\n",
    "\n",
    "If this completes sucessfully, you will now have a new directory called `envs/default` containing the specified environment. You can enter and exit this local env if desired as follows:\n",
    "\n",
    "\n",
    "```bash\n",
    "conda activate envs/default\n",
    "```\n",
    "\n",
    "With this local environment active, we can run the command `jupyter notebook` to test whether our two notebooks execute correctly. If all is well, we have completed the first cut at this project definition. In the next cut, we will make the project more robust and more user friendly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second cut at `anaconda-project.yml`\n",
    "\n",
    "We now have an environment definition that works, but we want to avoid having to tell users to activate environments and run special commands. In addition, we haven't pinned out packages which means we cannot guarantee the correct versions will be installed at a later date.\n",
    "\n",
    "### Pinning versions\n",
    "\n",
    "Assuming that our notebooks ran correctly when manually activating `envs/default` after `anaconda-project prepare`, we can look at the conda install logs for all the versions used. Here is what I found:\n",
    "\n",
    "```\n",
    "  - python=3.8.5\n",
    "  - notebook=6.1.1\n",
    "  - hvplot=0.6.0\n",
    "  - intake=0.6.0\n",
    "  - intake-parquet=0.2.3\n",
    "  - intake-xarray=0.3.1\n",
    "  - s3fs=0.5.0\n",
    "  - datashader=0.11.1\n",
    "  - python-snappy=0.5.4\n",
    "```\n",
    "\n",
    "We can simply replace the `packages` section of the `anaconda-project.yaml` with this block to pin all the package versions appropriately.\n",
    "\n",
    "\n",
    "### Defining commands\n",
    "\n",
    "When running `anaconda-project prepare` you may have noticed this warning:\n",
    "\n",
    "```\n",
    "Potential issues with this project:\n",
    "  * anaconda-project.yaml: No commands run notebooks Exploring_Data.ipynb, Reproducibly_Capturing_Code.ipynb\n",
    "```\n",
    "\n",
    "This is telling us that we should define commands for the users to run the notebooks, without having to activate the environment and running `jupyter notebook` themselves. To do this we insert the following into the `anaconda-project.yml`:\n",
    "\n",
    "\n",
    "```\n",
    "commands:\n",
    "  exploring:\n",
    "    notebook: Exploring_Data.ipynb\n",
    "  capturing:\n",
    "    notebook: Reproducibly_Capturing_Code.ipynb\n",
    "```\n",
    "\n",
    "This `commands` block specifies two targets called `exploring` and `capturing` allowing us to run:\n",
    "\n",
    "``` bash\n",
    "$ anaconda-project run  # OR\n",
    "$ anaconda-project run exploring # OR\n",
    "$ anaconda-project run capturing\n",
    "```\n",
    "\n",
    "Running `anaconda-project run` picks the first defined target which means it is equivalent to `anaconda-project run exploring`.\n",
    "\n",
    "\n",
    "# The final `anaconda-project.yml`\n",
    "\n",
    "Now we have everything to write our final `anaconda-project.yml` definition:\n",
    "\n",
    "\n",
    "```yaml\n",
    "# To reproduce: install 'anaconda-project', then 'anaconda-project run'\n",
    "name: exploring-data\n",
    "description: If you have a Pandas or Dask dataframe or an Xarray data\n",
    "             array, you can now get fully interactive, composable plots as simply as\n",
    "             \"df.hvplot()\".  We'll show how to use these tools for rapidly exploring\n",
    "             dastasets in Jupyter so that you can quickly spot outliers and\n",
    "             inconsistencies while revealing the big picture.\n",
    "\n",
    "channels: []\n",
    "\n",
    "packages:\n",
    "  - python=3.8.5\n",
    "  - notebook=6.1.1\n",
    "  - hvplot=0.6.0\n",
    "  - intake=0.6.0\n",
    "  - intake-parquet=0.2.3\n",
    "  - intake-xarray=0.3.1\n",
    "  - s3fs=0.5.0\n",
    "  - datashader=0.11.1\n",
    "  - python-snappy=0.5.4\n",
    "\n",
    "commands:\n",
    "  exploring:\n",
    "    notebook: Exploring_Data.ipynb\n",
    "  capturing:\n",
    "    notebook: Reproducibly_Capturing_Code.ipynb\n",
    "\n",
    "env_specs:\n",
    "  default: {}\n",
    "```\n",
    "\n",
    "The comment at the top now tells the user everything they need to run the project:\n",
    "\n",
    "``` bash\n",
    "$ conda install anaconda-project # If not already installed\n",
    "$ anaconda-project run\n",
    "```\n",
    "\n",
    "That's it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Archiving and distributing the project\n",
    "\n",
    "Now you have an `anaconda-project.yaml`, you want to get the project into the hands of users who can run it. The project consists of a directory containing several key files:\n",
    "\n",
    "- `anaconda-project.yml`: The project definition containing the name, description, environment and commands.\n",
    "- `*.ipynb`: The notebooks you want the user to run (`Exploring_Data.ipynb`, `Reproducibly_Capturing_Code.ipynb` in this case)\n",
    "- `diseases.csv.gz`: A supplementary data file needed by the `Exploring_Data.ipynb` notebook.\n",
    "\n",
    "We can now capture all these files into a single archive that can be given the users by running:\n",
    "\n",
    "```bash\n",
    "$ anaconda-project archive ../exploring_data.tar.gz\n",
    "```\n",
    "\n",
    "Now we can distribute `exploring_data.tar.gz` as a self-contained, reproducible project. The only reason to create the archive in the parent directory is so that `exploring_data.tar.gz` doesn't get captured if you run the archive command again. Note that the file size is low as `anaconda project` skips over the `envs/default` directory when packing the files together.\n",
    "\n",
    "## Uploading to anaconda.org\n",
    "\n",
    "If you have an [anconda.org](http://anaconda.org/) account, you can also run:\n",
    "\n",
    "```bash\n",
    "$ anaconda-project upload\n",
    "```\n",
    "\n",
    "Which will upload the archive and give you a shareable URL to access the project. This is how this very notebook was [distributed to you!](https://anaconda.org/jlstevens/project/exploring-data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next steps\n",
    "\n",
    "This notebook illustrates a minimal example of a project, namely to share a pair of notebooks. However, `anaconda-project` has many more features you can read about in the [docs](https://anaconda-project.readthedocs.io/) including:\n",
    "\n",
    "* The ability to define multiple environments per project (e.g a test environment in addition to a production environment).\n",
    "* The ability to define and pass environment variables to the processes run by commands.\n",
    "* The ability to fetch online data files before executing commands.\n",
    "* The ability to run arbitrary commands on both Unix systems and on Windows.\n",
    "\n",
    "Taken together, this makes `anaconda-project` very flexible and useable for a great deal more than simply running notebook servers. For instance, you can use it as a way of deploying on remote servers to run server processes, such as flask apps, REST APIs or interactive web-based dashboards. Using local environments makes it easy to set up `anaconda project` with CI: simply executing `anaconda-project run` commands will help ensure that the environments specified in the `anaconda-project.yml` are able to support the necessary commands.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
